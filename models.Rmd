---
title: "Machine Learning and Predictive Models"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = FALSE, message = FALSE, warning = FALSE, include = FALSE}
library(reshape2)
library(tigris)
library(ggplot2)
library(leaflet)
library(maps)
library(scales)
library(rgdal)
library(rgeos)
library(sqldf)

####General Notes####
# Use the section-picker at the bottom of the script window to jump between code sections
# Our analysis only considered the lower 48 states and the District of Columbia (excluded Alaska, Hawaii and overseas territories)
# The tigris data requires the below global option if you are running Windows 10
options(tigris_use_cache = FALSE)

####2016 General Election Data####
setwd("~/Second Year/DS 4559 - Data Science/Final Project 2/election2016/Data")

#Read the 2016 general election data + FIPS code database
gen16 <- read.csv("pres16results.csv", header = TRUE, stringsAsFactors = FALSE)
fips.labels <- read.csv("fips_database.csv", header = FALSE, stringsAsFactors = FALSE, colClasses = "character")

#Cleaning general election data
gen16 <- gen16[which(gen16$cand == "Donald Trump" | gen16$cand == "Hillary Clinton"),]
gen16 <- gen16[-c(1,2),]

sapply(gen16,class)

#This county wasn't labeled in the data, so we manually assigned it the county name
for (i in seq(nrow(gen16))){
  if (gen16$fips[i] == "46102") gen16$county[i] <- "Oglala Lakota County"
}

#Creating a separate table with just the state-wide results
gen16.states <- gen16[is.na(gen16$county),]
gen16.states <- gen16.states[-c(103,104),] #removing state FIPS for Alaska
gen16.states$county <- NULL
gen16.states$fips <- NULL

gen16 <- gen16[-is.na(gen16$county)]

#Removing the statewide results from the county data
for (i in seq(nrow(gen16))) {
  if (nchar(gen16$fips[i])<=2) gen16$fips[i] <- NA
}

gen16 <- gen16[-which(is.na(gen16$fips)),]
gen16$pct_report <- NULL
gen16.2 <- gen16

gen16.2$votes <- NULL
gen16.2$lead <- NULL

#Used the reshape2 package to convert the dataframe from long to wide format
long16 <- dcast(gen16.2, fips + st + total_votes ~ cand, value.var = "pct")
colnames(long16) <- c("fips", "st", "total_votes", "DonaldTrump", "HillaryClinton")
long16$diff <- long16$DonaldTrump - long16$HillaryClinton

#Added an extra leading 0 for FIPS codes that were 4 digits long (enables the merge later)
for (i in seq(nrow(long16))) {
  if (nchar(long16$fips[i])==4) long16$fips[i] <- paste("0",long16$fips[i], sep="")
}

long16$TrumpWin <- ifelse(long16$diff >0, long16$TrumpWin <- 1, long16$TrumpWin <- 0)
long16$TrumpWin <- as.factor(long16$TrumpWin)

####Cleaning FIPS database####
fips.labels$fips <- paste(fips.labels$V2,fips.labels$V3, sep = "")
fips.labels$V2 <- NULL
fips.labels$V3 <- NULL
fips.labels$V5 <- NULL
colnames(fips.labels) <- c("State", "County", "FIPS")

####Merge FIPS database with 2016 election results####
long16 <- merge(long16, fips.labels[-1], by.x = "fips", by.y = "FIPS", all.x = TRUE)

####2012 General Election Data####
#Data import
all_counties_2012 <- read.csv("~/Second Year/DS 4559 - Data Science/Final Project 2/election2016/Data/2012data/all_counties_2012.csv",
                              stringsAsFactors = FALSE)
all_counties_2012 <- subset(all_counties_2012, fips!="fips")
all_counties_2012$votes <- as.numeric(all_counties_2012$votes)
#case insensitive search for rows containing obama or romney
all_counties_2012_romney <- all_counties_2012[grepl('romney',all_counties_2012$candidate,ignore.case=TRUE), ] 
all_counties_2012_obama <- all_counties_2012[grepl('obama',all_counties_2012$candidate,ignore.case=TRUE), ] 

#all_counties_2012_romney <- aggregate (. ~ fips, data=all_counties_2012_romney, FUN=sum)

all_counties_2012 <- rbind(all_counties_2012_romney,all_counties_2012_obama)
all_counties_2012 <- subset(all_counties_2012, fips !="")
summary(all_counties_2012)
summary(all_counties_2012$fips)

#include only first letter
all_counties_2012[,3] <- substring(all_counties_2012[,3], 1, 1) 

all_counties_2012$candidate <- replace(all_counties_2012$candidate, all_counties_2012$candidate=="o", "obama")
all_counties_2012$candidate <- replace(all_counties_2012$candidate, all_counties_2012$candidate=="O", "obama")
all_counties_2012$candidate <- replace(all_counties_2012$candidate, all_counties_2012$candidate=="B", "obama")
all_counties_2012$candidate <- replace(all_counties_2012$candidate, all_counties_2012$candidate=="b", "obama")
all_counties_2012$candidate <- replace(all_counties_2012$candidate, all_counties_2012$candidate=="m", "romney")
all_counties_2012$candidate <- replace(all_counties_2012$candidate, all_counties_2012$candidate=="M", "romney")
all_counties_2012$candidate <- replace(all_counties_2012$candidate, all_counties_2012$candidate=="r", "romney")
all_counties_2012$candidate <- replace(all_counties_2012$candidate, all_counties_2012$candidate=="R", "romney")
all_counties_2012 <- aggregate (votes ~ fips+candidate+county, data=all_counties_2012, FUN=sum)

summary(all_counties_2012$candidate) 

all_counties_2012 <- data.frame(lapply(all_counties_2012, as.character), stringsAsFactors=FALSE)
all_counties_2012$votes <- as.numeric(all_counties_2012$votes)

long12 <- dcast(all_counties_2012, fips ~ candidate, value.var = "votes")

for (i in seq(nrow(long12))){
  if (long12$fips[i] == "46113") long12$fips[i] <- "46102"
}

long12$diff <- long12$obama - long12$romney

long12$ObamaWin <- ifelse(long12$diff >0, long12$ObamaWin <- 1, long12$ObamaWin <- 0)
long12$ObamaWin <- as.factor(long12$ObamaWin)

#Note that the percentages calculated do not take into account third-party votes (marginal)
long12$ObamaPer <- long12$obama/(long12$obama + long12$romney)
long12$RomneyPer <- long12$romney/(long12$obama + long12$romney)
long12$diffper <- long12$ObamaPer - long12$RomneyPer

long12 <- merge(long12, fips.labels, by.x = "fips", by.y = "FIPS", all.x = TRUE)

####Lower 48 State US Geographic Data####
us.counties <- counties(c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME",
                          "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", 
                          "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"), cb = TRUE)

us.counties2 <- fortify(us.counties, region = "GEOID")

us.states <- states(cb = TRUE)
us.states2 <- fortify(us.states, region = "GEOID")
us.states3 <- us.states2[which(us.states2$lat >= 24.396308 & us.states2$lat <= 49.384358 & us.states2$long >= -124.848974 & us.states2$long <= -66.885444),]
df_merged.us <- merge(us.counties2, long16, by.x = "id", by.y = "fips", all.x = TRUE)
```

### Variable Importance, 2012
Let's take a look at variable importance with regards to US Census data (per-county basis) for 2012. Note the high importance of the attribute measuring the proportion of the county that was considered 'Asian.'

```{r, echo = FALSE, warning=FALSE, message=FALSE}
census <- read.csv("C:/Users/Nathan/OneDrive/OneDrive Documents/Second Year/DS 4559 - Data Science/Final Project 2/election2016/Data/county_facts.csv", header = TRUE, stringsAsFactors = TRUE)
#check for unclean data
data <- subset(census,select=c("fips", "area_name", "state_abbreviation", "POP060210","PST040210","AGE775214","SEX255214","RHI225214", "RHI325214", "RHI425214", "RHI525214", "RHI725214", "RHI825214", "EDU635213", "EDU685213", "INC110213", "PVY020213"))
data <- subset(data, fips !=0) #remove USA
data <- subset(data, state_abbreviation !="") #remove states
colnames(data) <- c("fips", "area_name", "state_abbreviation", "pop_sqr_mile2010","pop_total_2010","pop_65+","p_female","p_black", "p_indian", "p_asian", "p_PI", "p_hisp", "p_white", "p_HS", "p_bachelors", "median_Income", "p_below_poverty_line")
data$fips <- as.factor(data$fips)
#set factors correctly

#symnum(cor(data[,4:17], use="complete.obs"))
#originally, we had both median income & percent below poverty line. since these had a 60% correlation we chose to keep just median income
data <- data[,1:16]

#import cleaned election data
clean_2012 <- read.csv("C:/Users/Nathan/OneDrive/OneDrive Documents/Second Year/DS 4559 - Data Science/Final Project 2/election2016/Data/clean_2012.csv")
clean_2012 <- clean_2012[,c(2,6)]
clean_2012$fips <- as.factor(clean_2012$fips)

clean_2016 <- read.csv("C:/Users/Nathan/OneDrive/OneDrive Documents/Second Year/DS 4559 - Data Science/Final Project 2/election2016/Data/clean_2016.csv")
clean_2016 <- clean_2016[,c(2,8)]
#Fix Ogala Lakota County FIPS code
clean_2016 [5359,]$fips <- 46113
clean_2016 [5360,]$fips <- 46113
clean_2016$fips <- as.factor(clean_2016$fips)

data_2012 <- merge(data, clean_2012,by="fips")

data_2016 <- merge(data, clean_2016,by="fips")
data_2016 <- data_2016[seq(1, 6223, 2),]

#Randomly order the data
set.seed(5)
data_2012 <- data_2012[order(runif(3112)), ]
data_2012$ObamaWin <- as.factor(data_2012$ObamaWin)

data_2016 <- data_2016[order(runif(3112)), ]

#Split the data ~80% training and 20% testing
train_2012 <- data_2012[1:2489,]
test_2012<- data_2012[2489:3112,]

train_2016 <- data_2016[1:2489,]
test_2016<- data_2016[2489:3112,]

#One way to look at attribute importance

library(caret)
library(corrplot)
library(DMwR)
library(ggplot2)
library(reshape2)
library(plyr)
library(sqldf)
library(mlbench)
library(randomForest)
library(gmodels)
library(party)
library(C50)
library(RWeka)
library(tigris)

model <- train(ObamaWin ~., data=train_2012[,4:17], method="lvq", preProcess="scale")#, trControl=control)
#Estimating variable importance
importance <- varImp(model, scale=FALSE)
#print(importance)
plot(importance, ylab = 'Attributes', main = 'Attribute Importance')
```

***

### Variable Importance, 2016
We can perform the same importance analysis for 2016.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5)
model <- train(lead ~., data=train_2016[,4:17], method="lvq", preProcess="scale")#, trControl=control)
#Estimating variable importance
importance <- varImp(model, scale=FALSE)
#print(importance)
plot(importance, ylab = 'Attributes', main = 'Attribute Importance')
```

***

### Conditional Inference Tree
We can build a conditional inference machine learning model to predict county-level outcomes on the 2012 data. In this case, our accuracy was **86.4%** on our test data set (separated from training data). As a note, accuracy is measured by summing the top-left cell and the bottom-right cell (along the lower-right diagonal).

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5)
tree_model = ctree(ObamaWin ~ ., train_2012[,4:17]) 
#plot(tree_model)
ctree_pred <- predict(tree_model,test_2012[,4:17])
CrossTable(test_2012$ObamaWin, ctree_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Type', 'Predicted Type'))
```

We can use this model on 2016 data as well. Here, our accuracy decreased on testing data to **71.65%**.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5)
ctree_pred2 <- predict(tree_model,test_2016[,4:17])
CrossTable(test_2016$lead, ctree_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Type', 'Predicted Type'))
```

***

### Sequential Covering and Rule-based Learning
We used a rule-based method (`jRip`) to model the 2012 data. Here, our accuracy in predicting county-level outcomes was **85.3%**. We've also listed the rules below. Our successes with rule-based sequential covering methods (compared to conditional inference trees) suggests a lower level of complexity with regards to individual voting decisions.

For reference, the general syntax for the classification rules is:
`rule-set => prediction (# of instances correctly classified by rule, # of instances misclassified by rule)`.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5)
jrip_model <- JRip(ObamaWin ~ ., train_2012[,4:17])
jrip_model

jrip_pred <- predict(jrip_model,test_2012[,4:17])
jrip_pred2 <- predict(jrip_model,test_2016[,4:17])

#2012 accuracy: 85.3
CrossTable(test_2012$ObamaWin, jrip_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Type', 'Predicted Type'))
```

Interestingly, when we apply this model trained on 2012 data to the 2016 data, our accuracy on test data increases to **93.5%** (or in other words, we are able to correctly predict county-level outcomes 93.5% of the time)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5)
CrossTable(test_2016$lead, jrip_pred2,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Type', 'Predicted Type'))

jrip_model2 <- JRip(lead ~ ., train_2016[,4:17])
jrip_model2
```

If we train a model and test it with purely 2016 data, interestingly, our accuracy decreases to **92%**.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5)
jrip_pred3 <- predict(jrip_model2,test_2016[,4:17])
CrossTable(test_2016$lead, jrip_pred3,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Type', 'Predicted Type'))
```

***

### kNN
We also used a k-nearest-neighbor algorithm for 2012 and 2016 data. With 2012 data, we were able to achieve an accuracy of **86.8%**.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5)
data12_knn <- data_2012[,4:17]
data16_knn <- data_2016[,4:17]


## We must ALWAYS normalize data before using the kNN algorithm.  Why?
## Here, we write a function to normalize any vector of variables, x.
normalize <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
}

## Now, we divide the data into testin and training data (just the attributes)
prc_train <- as.data.frame(lapply(data12_knn[1:2489,1:13], normalize))
prc_test <- as.data.frame(lapply(data12_knn[2490:3112,1:13], normalize))

## We make separate vectors for the classes for training and testing that correspond to the 
## matrices above:


prc_train_labels <- data12_knn[1:2489,14]
prc_test_labels <- data12_knn[2490:3112,14]

## "class" is the package that allows us to perform kNN analysis
library(class)

## Here we perform kNN analysis.  k= 15
prc_test_pred <- knn(train=prc_train,test=prc_test,cl=prc_train_labels,k=15)

## Evaluate

library(gmodels)
CrossTable(x=prc_test_labels, y=prc_test_pred,prop.chisq = FALSE)
```

Using 2016 training and testing data, our accuracy on a county-level basis increased to **93.57%**.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5)
prc_train <- as.data.frame(lapply(data16_knn[1:2489,1:13], normalize))
prc_test <- as.data.frame(lapply(data16_knn[2490:3112,1:13], normalize))
prc_train_labels <- data16_knn[1:2489,14]
prc_test_labels <- data16_knn[2490:3112,14]

## "class" is the package that allows us to perform kNN analysis
library(class)

## Here we perform kNN analysis.  k= 15
prc_test_pred <- knn(train=prc_train,test=prc_test,cl=prc_train_labels,k=15)

## Evaluate

library(gmodels)
CrossTable(x=prc_test_labels, y=prc_test_pred,prop.chisq = FALSE)
```

***

### Prediction Mapping
We can use our `jRip` model on all of the 2016 data, not just the test data, with an accuracy of **93%** (note the potential for bias due to the fact that the training data is being recycled for the prediction).

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=16, fig.height = 8}
set.seed(5)
#Predicting all of 2016 results using 2012
jrip_model_whole <- JRip(ObamaWin ~ ., data_2012[,4:17])
jrip_model_whole

jrip_pred_whole <- predict(jrip_model_whole,data_2016[,4:17])
CrossTable(data_2016$lead, jrip_pred_whole,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Type', 'Predicted Type'))
```

##### 2016 Prediction Map
We can map the result of our `jRip` model and compare it to the actual 2016 result.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=16, fig.height = 8}
options(tigris_use_cache = FALSE)
us.counties <- counties(c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "FL", "GA", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME",
                          "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", 
                          "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"), cb = TRUE)

#Fortify the spacial polygon to convert to dataframe format (for ggplot2)
us.counties2 <- fortify(us.counties, region = "GEOID")

#Obtained state boundaries so we can overlay a white boundary/outline over the US map
us.states <- states(cb = TRUE)
us.states2 <- fortify(us.states, region = "GEOID")
#Limited the state dataframe to exclude areas outside the 48 states (territories, AK, HI)
us.states3 <- us.states2[which(us.states2$lat >= 24.396308 & us.states2$lat <= 49.384358 & us.states2$long >= -124.848974 & us.states2$long <= -66.885444),]

map2016 <- data_2016[c(1:3)]
map2016 <- cbind(map2016, jrip_pred_whole)
map2016$fips <- as.character(map2016$fips)
map2016$area_name <- as.character(map2016$area_name)

#Add leading 0 to 4 digit FIPS codes
for (i in seq(nrow(map2016))) {
  if (nchar(map2016$fips[i])==4) map2016$fips[i] <- paste("0",map2016$fips[i], sep="")
}

#Fix Oglala Lakota County
for (i in seq(nrow(map2016))) {
  if (map2016$fips[i] == "46113") map2016$fips[i] <- "46102"
}

#Add in actual results for a comparision
map2016$actual <- ifelse(data_2016$lead == "Donald Trump", 1, 2)
map2016$jrip_pred_whole <- as.factor(map2016$jrip_pred_whole)

map2016$acc <- ifelse(map2016$jrip_pred_whole == 0 & map2016$actual == 2, 3, 
                           ifelse(map2016$jrip_pred_whole == 1 & map2016$actual == 1, 4, 0))
map2016$acc <- as.factor(map2016$acc)

#JRIP Prediction Graphs
df_merged.jrip <- merge(us.counties2, map2016, by.x = "id", by.y = "fips", all.x = TRUE)

us.jrip <- ggplot() +
  geom_polygon(data = df_merged.jrip, aes(x = long, y = lat, group = group, fill = jrip_pred_whole), color = "dark grey", size = 0.25) +
  geom_path(data = us.states3, aes(x=long, y=lat, group =group), color = "white", size = 1) +
  scale_fill_manual(values = c("red","blue"), labels=c("Trump", "Clinton"),name="Prediction County Winner") + 
  ggtitle("2016 Electoral Map by County, JRIP Model Prediction") + coord_map("polyconic") + theme_void()
us.jrip
```

##### 2016 Actual Map
```{r fig.width=16, fig.height = 8, echo = FALSE}
us.ggmap2 <- ggplot() +
  geom_polygon(data = df_merged.us, aes(x = long, y = lat, group = group, fill = TrumpWin), color = "dark grey", size = 0.25) +
  geom_path(data = us.states3, aes(x=long, y=lat, group =group), color = "white", size = 1) +
  scale_fill_manual(values = c("blue","red"), labels=c("Clinton", "Trump"),name="County Winner") + 
  ggtitle("2016 Electoral Map by County") + coord_map("polyconic") + theme_void() 
us.ggmap2
```

##### Prediction Error Visualized
We can actually visualize our model's prediction error. The counties in grey represent correct predictions, while blue highlights represent Trump predictions that actually voted for Clinton and red highlights represent Clinton predictions that actually voted for Trump.

Additionally, while these models are able to predict county-level results on a win/loss basis, they are unable to predict Clinton/Trump votes in the area.

```{r fig.width=16, fig.height = 8, echo = FALSE}
us.jrip2 <- ggplot() +
  geom_polygon(data = df_merged.jrip, aes(x = long, y = lat, group = group, fill = acc), color = "dark grey", size = 0.25) +
  geom_path(data = us.states3, aes(x=long, y=lat, group =group), color = "white") +
  scale_fill_manual(values = c("grey", "blue", "red"), labels=c("Correct", "Actual Clinton", "Actual Trump"),name="JRIP Error") + 
  ggtitle("2016 Electoral Map by County, JRIP Model Prediction Accuracy") + coord_map("polyconic") + theme_void() 
us.jrip2
```